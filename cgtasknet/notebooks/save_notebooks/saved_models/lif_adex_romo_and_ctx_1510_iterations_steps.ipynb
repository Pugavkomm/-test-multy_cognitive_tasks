{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two task network \n",
    "\n",
    "Network has eight inputs:\n",
    "\n",
    "1. The fixation. \n",
    "1. $u_{rule}^{1}$\n",
    "1. $u_{rule}^{2}$\n",
    "1. The first context mod. \n",
    "1. The second ontext mod. \n",
    "1. The first context status. \n",
    "1. The second context status. \n",
    "1. The Romo signals.\n",
    "\n",
    "Network has five outputs: \n",
    "1. The fixation. \n",
    "1. The first context output. \n",
    "1. The second context output. \n",
    "1. The first Romo task output. \n",
    "1. The second Romo task output. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"./images/Sheme.png\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "> Learning rule: superspike\n",
    "\n",
    "> Neuron type: Alif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt  # for analys\n",
    "from cgtasknet.net.lifadex import SNNlifadex\n",
    "from cgtasknet.tasks.tasks import MultyTask\n",
    "from norse.torch.functional.lif_adex import LIFAdExParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step -1: Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: gpu (cuda)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(f'Device: {(\"gpu (cuda)\" if device.type==\"cuda\" else \"cpu\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "number_of_tasks = 8\n",
    "task_list = [(\"WorkingMemory\", dict()), ((\"ContextDM\", dict()))]\n",
    "tasks = dict(task_list)\n",
    "Task = MultyTask(tasks=tasks, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size, output_size = Task.feature_and_act_size[0]\n",
    "hidden_size = 400\n",
    "\n",
    "neuron_parameters = LIFAdExParameters(\n",
    "    tau_ada_inv=torch.Tensor([2]).to(device), alpha=100\n",
    ")\n",
    "model = SNNlifadex(\n",
    "    feature_size, hidden_size, output_size, neuron_parameters=neuron_parameters\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Save pre-learning weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_pre_l = []\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            weights_pre_l.append((param).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: loss and creterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.8, 0.85))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.8, 0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "epoch: 10 loss: 0.20364\n",
      "test loss: 0.13287\n",
      "epoch: 20 loss: 0.12520\n",
      "test loss: 0.09666\n",
      "epoch: 30 loss: 0.08820\n",
      "test loss: 0.07935\n",
      "epoch: 40 loss: 0.07431\n",
      "test loss: 0.06799\n",
      "epoch: 50 loss: 0.06368\n",
      "test loss: 0.05246\n",
      "epoch: 60 loss: 0.05657\n",
      "test loss: 0.04151\n",
      "epoch: 70 loss: 0.05446\n",
      "test loss: 0.05290\n",
      "epoch: 80 loss: 0.05143\n",
      "test loss: 0.05604\n",
      "epoch: 90 loss: 0.04889\n",
      "test loss: 0.04217\n",
      "epoch: 100 loss: 0.04849\n",
      "test loss: 0.04119\n",
      "epoch: 110 loss: 0.04408\n",
      "test loss: 0.03232\n",
      "epoch: 120 loss: 0.04482\n",
      "test loss: 0.04384\n",
      "epoch: 130 loss: 0.04106\n",
      "test loss: 0.04035\n",
      "epoch: 140 loss: 0.04204\n",
      "test loss: 0.04079\n",
      "epoch: 150 loss: 0.04011\n",
      "test loss: 0.04973\n",
      "epoch: 160 loss: 0.03872\n",
      "test loss: 0.03478\n",
      "epoch: 170 loss: 0.03822\n",
      "test loss: 0.03307\n",
      "epoch: 180 loss: 0.03957\n",
      "test loss: 0.03102\n",
      "epoch: 190 loss: 0.03875\n",
      "test loss: 0.04892\n",
      "epoch: 200 loss: 0.03857\n",
      "test loss: 0.04175\n",
      "epoch: 210 loss: 0.03825\n",
      "test loss: 0.02174\n",
      "epoch: 220 loss: 0.03568\n",
      "test loss: 0.04529\n",
      "epoch: 230 loss: 0.03567\n",
      "test loss: 0.03303\n",
      "epoch: 240 loss: 0.03646\n",
      "test loss: 0.04112\n",
      "epoch: 250 loss: 0.03441\n",
      "test loss: 0.04484\n",
      "epoch: 260 loss: 0.03580\n",
      "test loss: 0.03655\n",
      "epoch: 270 loss: 0.03074\n",
      "test loss: 0.03850\n",
      "epoch: 280 loss: 0.03362\n",
      "test loss: 0.02925\n",
      "epoch: 290 loss: 0.03402\n",
      "test loss: 0.04174\n",
      "epoch: 300 loss: 0.03499\n",
      "test loss: 0.02287\n",
      "epoch: 310 loss: 0.03726\n",
      "test loss: 0.04018\n",
      "epoch: 320 loss: 0.03178\n",
      "test loss: 0.03636\n",
      "epoch: 330 loss: 0.02962\n",
      "test loss: 0.03742\n",
      "epoch: 340 loss: 0.03745\n",
      "test loss: 0.02395\n",
      "epoch: 350 loss: 0.03001\n",
      "test loss: 0.02928\n",
      "epoch: 360 loss: 0.03273\n",
      "test loss: 0.03711\n",
      "epoch: 370 loss: 0.03603\n",
      "test loss: 0.04118\n",
      "epoch: 380 loss: 0.02943\n",
      "test loss: 0.01432\n",
      "epoch: 390 loss: 0.03676\n",
      "test loss: 0.02617\n",
      "epoch: 400 loss: 0.03652\n",
      "test loss: 0.03896\n",
      "epoch: 410 loss: 0.02939\n",
      "test loss: 0.02608\n",
      "epoch: 420 loss: 0.03300\n",
      "test loss: 0.03884\n",
      "epoch: 430 loss: 0.02808\n",
      "test loss: 0.03825\n",
      "epoch: 440 loss: 0.03413\n",
      "test loss: 0.01451\n",
      "epoch: 450 loss: 0.03406\n",
      "test loss: 0.03679\n",
      "epoch: 460 loss: 0.03168\n",
      "test loss: 0.04633\n",
      "epoch: 470 loss: 0.03557\n",
      "test loss: 0.02824\n",
      "epoch: 480 loss: 0.03170\n",
      "test loss: 0.03337\n",
      "epoch: 490 loss: 0.03494\n",
      "test loss: 0.01892\n",
      "epoch: 500 loss: 0.03368\n",
      "test loss: 0.03860\n",
      "epoch: 510 loss: 0.03548\n",
      "test loss: 0.02127\n",
      "epoch: 520 loss: 0.03246\n",
      "test loss: 0.02239\n",
      "epoch: 530 loss: 0.03443\n",
      "test loss: 0.02158\n",
      "epoch: 540 loss: 0.03053\n",
      "test loss: 0.02663\n",
      "epoch: 550 loss: 0.03289\n",
      "test loss: 0.02305\n",
      "epoch: 560 loss: 0.03184\n",
      "test loss: 0.04158\n",
      "epoch: 570 loss: 0.03040\n",
      "test loss: 0.02860\n",
      "epoch: 580 loss: 0.02805\n",
      "test loss: 0.03176\n",
      "epoch: 590 loss: 0.03323\n",
      "test loss: 0.03970\n",
      "epoch: 600 loss: 0.02416\n",
      "test loss: 0.02177\n",
      "epoch: 610 loss: 0.03361\n",
      "test loss: 0.03348\n",
      "epoch: 620 loss: 0.03146\n",
      "test loss: 0.03633\n",
      "epoch: 630 loss: 0.03304\n",
      "test loss: 0.03550\n",
      "epoch: 640 loss: 0.03159\n",
      "test loss: 0.03413\n",
      "epoch: 650 loss: 0.03072\n",
      "test loss: 0.03241\n",
      "epoch: 660 loss: 0.03349\n",
      "test loss: 0.04704\n",
      "epoch: 670 loss: 0.03250\n",
      "test loss: 0.04129\n",
      "epoch: 680 loss: 0.03203\n",
      "test loss: 0.03712\n",
      "epoch: 690 loss: 0.03151\n",
      "test loss: 0.03504\n",
      "epoch: 700 loss: 0.03243\n",
      "test loss: 0.04017\n",
      "epoch: 710 loss: 0.03557\n",
      "test loss: 0.02741\n",
      "epoch: 720 loss: 0.03245\n",
      "test loss: 0.03047\n",
      "epoch: 730 loss: 0.02880\n",
      "test loss: 0.01890\n",
      "epoch: 740 loss: 0.03302\n",
      "test loss: 0.03229\n",
      "epoch: 750 loss: 0.02844\n",
      "test loss: 0.03692\n",
      "epoch: 760 loss: 0.03343\n",
      "test loss: 0.03504\n",
      "epoch: 770 loss: 0.02916\n",
      "test loss: 0.02525\n",
      "epoch: 780 loss: 0.03353\n",
      "test loss: 0.02688\n",
      "epoch: 790 loss: 0.02912\n",
      "test loss: 0.02635\n",
      "epoch: 800 loss: 0.02809\n",
      "test loss: 0.02894\n",
      "epoch: 810 loss: 0.02716\n",
      "test loss: 0.04041\n",
      "epoch: 820 loss: 0.03123\n",
      "test loss: 0.02734\n",
      "epoch: 830 loss: 0.03133\n",
      "test loss: 0.03662\n",
      "epoch: 840 loss: 0.03032\n",
      "test loss: 0.03324\n",
      "epoch: 850 loss: 0.03222\n",
      "test loss: 0.02200\n",
      "epoch: 860 loss: 0.03284\n",
      "test loss: 0.02667\n",
      "epoch: 870 loss: 0.02932\n",
      "test loss: 0.03965\n",
      "epoch: 880 loss: 0.02828\n",
      "test loss: 0.03832\n",
      "epoch: 890 loss: 0.02948\n",
      "test loss: 0.02835\n",
      "epoch: 900 loss: 0.03435\n",
      "test loss: 0.02910\n",
      "epoch: 910 loss: 0.02810\n",
      "test loss: 0.02957\n",
      "epoch: 920 loss: 0.03179\n",
      "test loss: 0.03481\n",
      "epoch: 930 loss: 0.02621\n",
      "test loss: 0.03251\n",
      "epoch: 940 loss: 0.02881\n",
      "test loss: 0.03586\n",
      "epoch: 950 loss: 0.02974\n",
      "test loss: 0.02818\n",
      "epoch: 960 loss: 0.03110\n",
      "test loss: 0.02630\n",
      "epoch: 970 loss: 0.02772\n",
      "test loss: 0.02751\n",
      "epoch: 980 loss: 0.02609\n",
      "test loss: 0.03113\n",
      "epoch: 990 loss: 0.02657\n",
      "test loss: 0.02970\n",
      "epoch: 1000 loss: 0.03081\n",
      "test loss: 0.03224\n",
      "epoch: 1010 loss: 0.03413\n",
      "test loss: 0.01700\n",
      "epoch: 1020 loss: 0.03163\n",
      "test loss: 0.01970\n",
      "epoch: 1030 loss: 0.03110\n",
      "test loss: 0.03794\n",
      "epoch: 1040 loss: 0.02663\n",
      "test loss: 0.04549\n",
      "epoch: 1050 loss: 0.02724\n",
      "test loss: 0.02554\n",
      "epoch: 1060 loss: 0.02782\n",
      "test loss: 0.03435\n",
      "epoch: 1070 loss: 0.02858\n",
      "test loss: 0.02850\n",
      "epoch: 1080 loss: 0.02521\n",
      "test loss: 0.03099\n",
      "epoch: 1090 loss: 0.02725\n",
      "test loss: 0.02583\n",
      "epoch: 1100 loss: 0.02934\n",
      "test loss: 0.02936\n",
      "epoch: 1110 loss: 0.03033\n",
      "test loss: 0.02592\n",
      "epoch: 1120 loss: 0.02618\n",
      "test loss: 0.04720\n",
      "epoch: 1130 loss: 0.02925\n",
      "test loss: 0.03863\n",
      "epoch: 1140 loss: 0.02793\n",
      "test loss: 0.03484\n",
      "epoch: 1150 loss: 0.03602\n",
      "test loss: 0.02306\n",
      "epoch: 1160 loss: 0.02632\n",
      "test loss: 0.03986\n",
      "epoch: 1170 loss: 0.03169\n",
      "test loss: 0.03143\n",
      "epoch: 1180 loss: 0.02936\n",
      "test loss: 0.02626\n",
      "epoch: 1190 loss: 0.02998\n",
      "test loss: 0.02615\n",
      "epoch: 1200 loss: 0.03046\n",
      "test loss: 0.02622\n",
      "epoch: 1210 loss: 0.03057\n",
      "test loss: 0.02859\n",
      "epoch: 1220 loss: 0.02979\n",
      "test loss: 0.01157\n",
      "epoch: 1230 loss: 0.02669\n",
      "test loss: 0.01560\n",
      "epoch: 1240 loss: 0.02689\n",
      "test loss: 0.01233\n",
      "epoch: 1250 loss: 0.03340\n",
      "test loss: 0.02845\n",
      "epoch: 1260 loss: 0.02725\n",
      "test loss: 0.01187\n",
      "epoch: 1270 loss: 0.02987\n",
      "test loss: 0.02465\n",
      "epoch: 1280 loss: 0.02853\n",
      "test loss: 0.02880\n",
      "epoch: 1290 loss: 0.02760\n",
      "test loss: 0.01546\n",
      "epoch: 1300 loss: 0.02971\n",
      "test loss: 0.02105\n",
      "epoch: 1310 loss: 0.03121\n",
      "test loss: 0.04603\n",
      "epoch: 1320 loss: 0.03193\n",
      "test loss: 0.02790\n",
      "epoch: 1330 loss: 0.03424\n",
      "test loss: 0.02781\n",
      "epoch: 1340 loss: 0.03396\n",
      "test loss: 0.01382\n",
      "epoch: 1350 loss: 0.03403\n",
      "test loss: 0.02888\n",
      "epoch: 1360 loss: 0.02591\n",
      "test loss: 0.02456\n",
      "epoch: 1370 loss: 0.02985\n",
      "test loss: 0.03666\n",
      "epoch: 1380 loss: 0.03277\n",
      "test loss: 0.02316\n",
      "epoch: 1390 loss: 0.03480\n",
      "test loss: 0.03562\n",
      "epoch: 1400 loss: 0.03132\n",
      "test loss: 0.03019\n",
      "epoch: 1410 loss: 0.02861\n",
      "test loss: 0.02720\n",
      "epoch: 1420 loss: 0.02610\n",
      "test loss: 0.03607\n",
      "epoch: 1430 loss: 0.02991\n",
      "test loss: 0.01287\n",
      "epoch: 1440 loss: 0.02596\n",
      "test loss: 0.03344\n",
      "epoch: 1450 loss: 0.02961\n",
      "test loss: 0.03621\n",
      "epoch: 1460 loss: 0.02921\n",
      "test loss: 0.03936\n",
      "epoch: 1470 loss: 0.02680\n",
      "test loss: 0.01638\n",
      "epoch: 1480 loss: 0.02777\n",
      "test loss: 0.03685\n",
      "epoch: 1490 loss: 0.03086\n",
      "test loss: 0.03808\n",
      "epoch: 1500 loss: 0.02773\n",
      "test loss: 0.03102\n",
      "epoch: 1510 loss: 0.02978\n",
      "test loss: 0.04112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_420367/3248432162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "plt.ion\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig2 = plt.figure()\n",
    "\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.set_title(\"lifadex\")\n",
    "ax.set_title(\"lifadex\")\n",
    "inputs, target_outputs = Task.dataset(number_of_tasks)\n",
    "(line1,) = ax.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 1], \"b-\")\n",
    "(line2,) = ax.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 2], \"r-\")\n",
    "(line3,) = ax.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 1], \"b-\")\n",
    "(line4,) = ax.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 2], \"r-\")\n",
    "(line21,) = ax2.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 1], \"b-\")\n",
    "(line22,) = ax2.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 2], \"r-\")\n",
    "(line23,) = ax2.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 1], \"b-\")\n",
    "(line24,) = ax2.plot(np.arange(0, len(target_outputs)), target_outputs[:, 0, 2], \"r-\")\n",
    "ax.set_ylim([-0.5, 1.5])\n",
    "ax.set_xlim([0, 20000])\n",
    "ax2.set_ylim([-0.5, 1.5])\n",
    "ax2.set_xlim([0, 20000])\n",
    "running_loss = 0\n",
    "fig.canvas.draw()\n",
    "fig.canvas.flush_events()\n",
    "fig2.canvas.draw()\n",
    "fig2.canvas.flush_events()\n",
    "for i in range(2000):\n",
    "    inputs, target_outputs = Task.dataset(number_of_tasks)\n",
    "    inputs += np.random.normal(0, 0.01, size=(inputs.shape))\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n",
    "    target_outputs = torch.from_numpy(target_outputs).type(torch.float).to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs, states = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, target_outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 10 == 9:\n",
    "        print(\"epoch: {:d} loss: {:0.5f}\".format(i + 1, running_loss / 10))\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            inputs, target_outputs = Task.dataset(number_of_tasks)\n",
    "\n",
    "            inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n",
    "            target_outputs = (\n",
    "                torch.from_numpy(target_outputs).type(torch.float).to(device)\n",
    "            )\n",
    "            outputs, states = model(inputs)\n",
    "            loss = criterion(outputs, target_outputs)\n",
    "\n",
    "            print(\"test loss: {:0.5f}\".format(loss.item()))\n",
    "        for_plot = outputs.detach().cpu().numpy()[:, 0, :]\n",
    "        line1.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line2.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line3.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line4.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line21.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line22.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line23.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "        line24.set_xdata(np.arange(0, len(for_plot), 1))\n",
    "\n",
    "        line1.set_ydata(for_plot[:, 1])\n",
    "        line2.set_ydata(for_plot[:, 2])\n",
    "        line3.set_ydata(target_outputs.detach().cpu().numpy()[:, 0, 1])\n",
    "        line4.set_ydata(target_outputs.detach().cpu().numpy()[:, 0, 2])\n",
    "\n",
    "        line21.set_ydata(for_plot[:, 3])\n",
    "        line22.set_ydata(for_plot[:, 4])\n",
    "        line23.set_ydata(target_outputs.detach().cpu().numpy()[:, 0, 3])\n",
    "        line24.set_ydata(target_outputs.detach().cpu().numpy()[:, 0, 4])\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()\n",
    "    fig2.canvas.draw()\n",
    "    fig2.canvas.flush_events()\n",
    "\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lif_adex_romo_and_ctx_1510_iterations_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f83ba25b4055e6850166001f95ed136092a6ce41bf7679ca537d862fe029930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
