{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 4525.3115234375\n",
      "199 3007.236083984375\n",
      "299 1999.9383544921875\n",
      "399 1331.40234375\n",
      "499 887.5914306640625\n",
      "599 592.8893432617188\n",
      "699 397.1455078125\n",
      "799 267.09283447265625\n",
      "899 180.65875244140625\n",
      "999 123.19529724121094\n",
      "1099 84.97883605957031\n",
      "1199 59.5537223815918\n",
      "1299 42.63179016113281\n",
      "1399 31.364652633666992\n",
      "1499 23.85948371887207\n",
      "1599 18.857807159423828\n",
      "1699 15.52299690246582\n",
      "1799 13.298460006713867\n",
      "1899 11.813699722290039\n",
      "1999 10.822175979614258\n",
      "Result: y = -0.020060185343027115 + 0.8173043131828308 x + 0.0034607136622071266 x^2 + -0.08772088587284088 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f\"Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(0.9803, grad_fn=<PowBackward0>)\n",
      "tensor(0.9232, grad_fn=<PowBackward0>)\n",
      "tensor(0.8332, grad_fn=<PowBackward0>)\n",
      "tensor(0.7172, grad_fn=<PowBackward0>)\n",
      "tensor(0.5843, grad_fn=<PowBackward0>)\n",
      "tensor(0.4448, grad_fn=<PowBackward0>)\n",
      "tensor(0.3096, grad_fn=<PowBackward0>)\n",
      "tensor(0.1892, grad_fn=<PowBackward0>)\n",
      "tensor(0.0931, grad_fn=<PowBackward0>)\n",
      "tensor(0.0286, grad_fn=<PowBackward0>)\n",
      "tensor(0.0009, grad_fn=<PowBackward0>)\n",
      "tensor(0.0121, grad_fn=<PowBackward0>)\n",
      "tensor(0.0612, grad_fn=<PowBackward0>)\n",
      "tensor(0.1445, grad_fn=<PowBackward0>)\n",
      "tensor(0.2556, grad_fn=<PowBackward0>)\n",
      "tensor(0.3856, grad_fn=<PowBackward0>)\n",
      "tensor(0.5246, grad_fn=<PowBackward0>)\n",
      "tensor(0.6617, grad_fn=<PowBackward0>)\n",
      "tensor(0.7861, grad_fn=<PowBackward0>)\n",
      "tensor(0.8883, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float32\n",
    "x = torch.tensor(1.0, dtype=dtype)\n",
    "y_learn = torch.tensor(2.0, dtype=dtype)\n",
    "w = torch.tensor(1.0, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "number_of_interation = 2000\n",
    "for i in range(number_of_interation):\n",
    "    y_pred = w * x\n",
    "\n",
    "    loss = (y_learn - y_pred) ** 2\n",
    "\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    if i % 99 == 0:\n",
    "        print(loss)\n",
    "    loss.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.000000\n",
      "epoch 2: w = 1.680, loss = 4.799999\n",
      "epoch 3: w = 1.872, loss = 0.768000\n",
      "epoch 4: w = 1.949, loss = 0.122880\n",
      "epoch 5: w = 1.980, loss = 0.019661\n",
      "epoch 6: w = 1.992, loss = 0.003146\n",
      "epoch 7: w = 1.997, loss = 0.000503\n",
      "epoch 8: w = 1.999, loss = 0.000081\n",
      "epoch 9: w = 1.999, loss = 0.000013\n",
      "epoch 10: w = 2.000, loss = 0.000002\n",
      "epoch 11: w = 2.000, loss = 0.000000\n",
      "epoch 12: w = 2.000, loss = 0.000000\n",
      "epoch 13: w = 2.000, loss = 0.000000\n",
      "epoch 14: w = 2.000, loss = 0.000000\n",
      "epoch 15: w = 2.000, loss = 0.000000\n",
      "epoch 16: w = 2.000, loss = 0.000000\n",
      "epoch 17: w = 2.000, loss = 0.000000\n",
      "epoch 18: w = 2.000, loss = 0.000000\n",
      "epoch 19: w = 2.000, loss = 0.000000\n",
      "epoch 20: w = 2.000, loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = 2 * np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "w = 0.0\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2 * x, y_predicted - y).mean()\n",
    "\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "number_of_interation = 20\n",
    "\n",
    "for epoch in range(number_of_interation):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(Y, y_pred)\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    w -= learning_rate * dw\n",
    "    print(f\"epoch {epoch + 1}: w = {w:.3f}, loss = {l:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.000000\n",
      "epoch 11: w = 1.665, loss = 1.162786\n",
      "epoch 21: w = 1.934, loss = 0.045069\n",
      "epoch 31: w = 1.987, loss = 0.001747\n",
      "epoch 41: w = 1.997, loss = 0.000068\n",
      "epoch 51: w = 1.999, loss = 0.000003\n",
      "epoch 61: w = 2.000, loss = 0.000000\n",
      "epoch 71: w = 2.000, loss = 0.000000\n",
      "epoch 81: w = 2.000, loss = 0.000000\n",
      "epoch 91: w = 2.000, loss = 0.000000\n",
      "epoch 101: w = 2.000, loss = 0.000000\n",
      "epoch 111: w = 2.000, loss = 0.000000\n",
      "epoch 121: w = 2.000, loss = 0.000000\n",
      "epoch 131: w = 2.000, loss = 0.000000\n",
      "epoch 141: w = 2.000, loss = 0.000000\n",
      "epoch 151: w = 2.000, loss = 0.000000\n",
      "epoch 161: w = 2.000, loss = 0.000000\n",
      "epoch 171: w = 2.000, loss = 0.000000\n",
      "epoch 181: w = 2.000, loss = 0.000000\n",
      "epoch 191: w = 2.000, loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = 2 * X\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "def loss(y: torch.tensor, y_pred: torch.tensor):\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "number_of_interation = 200\n",
    "\n",
    "\n",
    "for epoch in range(number_of_interation):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(Y, y_pred)\n",
    "    l.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    w.grad.zero_()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch + 1}: w = {w:.3f}, loss = {l:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 2.308\n",
      "Prediction after training: f(5) = 2.308\n",
      "epoch 1: w = 3.029, loss = 19.873919\n",
      "epoch 11: w = 2.055, loss = 0.008711\n",
      "epoch 21: w = 2.028, loss = 0.001144\n",
      "epoch 31: w = 2.020, loss = 0.000621\n",
      "epoch 41: w = 2.015, loss = 0.000338\n",
      "epoch 51: w = 2.011, loss = 0.000184\n",
      "epoch 61: w = 2.008, loss = 0.000100\n",
      "epoch 71: w = 2.006, loss = 0.000055\n",
      "epoch 81: w = 2.004, loss = 0.000030\n",
      "epoch 91: w = 2.003, loss = 0.000016\n",
      "epoch 101: w = 2.002, loss = 0.000009\n",
      "epoch 111: w = 2.002, loss = 0.000005\n",
      "epoch 121: w = 2.001, loss = 0.000003\n",
      "epoch 131: w = 2.001, loss = 0.000001\n",
      "epoch 141: w = 2.001, loss = 0.000001\n",
      "epoch 151: w = 2.001, loss = 0.000000\n",
      "epoch 161: w = 2.000, loss = 0.000000\n",
      "epoch 171: w = 2.000, loss = 0.000000\n",
      "epoch 181: w = 2.000, loss = 0.000000\n",
      "epoch 191: w = 2.000, loss = 0.000000\n",
      "epoch 201: w = 2.000, loss = 0.000000\n",
      "epoch 211: w = 2.000, loss = 0.000000\n",
      "epoch 221: w = 2.000, loss = 0.000000\n",
      "epoch 231: w = 2.000, loss = 0.000000\n",
      "epoch 241: w = 2.000, loss = 0.000000\n",
      "epoch 251: w = 2.000, loss = 0.000000\n",
      "epoch 261: w = 2.000, loss = 0.000000\n",
      "epoch 271: w = 2.000, loss = 0.000000\n",
      "epoch 281: w = 2.000, loss = 0.000000\n",
      "epoch 291: w = 2.000, loss = 0.000000\n",
      "epoch 301: w = 2.000, loss = 0.000000\n",
      "epoch 311: w = 2.000, loss = 0.000000\n",
      "epoch 321: w = 2.000, loss = 0.000000\n",
      "epoch 331: w = 2.000, loss = 0.000000\n",
      "epoch 341: w = 2.000, loss = 0.000000\n",
      "epoch 351: w = 2.000, loss = 0.000000\n",
      "epoch 361: w = 2.000, loss = 0.000000\n",
      "epoch 371: w = 2.000, loss = 0.000000\n",
      "epoch 381: w = 2.000, loss = 0.000000\n",
      "epoch 391: w = 2.000, loss = 0.000000\n",
      "epoch 401: w = 2.000, loss = 0.000000\n",
      "epoch 411: w = 2.000, loss = 0.000000\n",
      "epoch 421: w = 2.000, loss = 0.000000\n",
      "epoch 431: w = 2.000, loss = 0.000000\n",
      "epoch 441: w = 2.000, loss = 0.000000\n",
      "epoch 451: w = 2.000, loss = 0.000000\n",
      "epoch 461: w = 2.000, loss = 0.000000\n",
      "epoch 471: w = 2.000, loss = 0.000000\n",
      "epoch 481: w = 2.000, loss = 0.000000\n",
      "epoch 491: w = 2.000, loss = 0.000000\n",
      "epoch 501: w = 2.000, loss = 0.000000\n",
      "epoch 511: w = 2.000, loss = 0.000000\n",
      "epoch 521: w = 2.000, loss = 0.000000\n",
      "epoch 531: w = 2.000, loss = 0.000000\n",
      "epoch 541: w = 2.000, loss = 0.000000\n",
      "epoch 551: w = 2.000, loss = 0.000000\n",
      "epoch 561: w = 2.000, loss = 0.000000\n",
      "epoch 571: w = 2.000, loss = 0.000000\n",
      "epoch 581: w = 2.000, loss = 0.000000\n",
      "epoch 591: w = 2.000, loss = 0.000000\n",
      "epoch 601: w = 2.000, loss = 0.000000\n",
      "epoch 611: w = 2.000, loss = 0.000000\n",
      "epoch 621: w = 2.000, loss = 0.000000\n",
      "epoch 631: w = 2.000, loss = 0.000000\n",
      "epoch 641: w = 2.000, loss = 0.000000\n",
      "epoch 651: w = 2.000, loss = 0.000000\n",
      "epoch 661: w = 2.000, loss = 0.000000\n",
      "epoch 671: w = 2.000, loss = 0.000000\n",
      "epoch 681: w = 2.000, loss = 0.000000\n",
      "epoch 691: w = 2.000, loss = 0.000000\n",
      "epoch 701: w = 2.000, loss = 0.000000\n",
      "epoch 711: w = 2.000, loss = 0.000000\n",
      "epoch 721: w = 2.000, loss = 0.000000\n",
      "epoch 731: w = 2.000, loss = 0.000000\n",
      "epoch 741: w = 2.000, loss = 0.000000\n",
      "epoch 751: w = 2.000, loss = 0.000000\n",
      "epoch 761: w = 2.000, loss = 0.000000\n",
      "epoch 771: w = 2.000, loss = 0.000000\n",
      "epoch 781: w = 2.000, loss = 0.000000\n",
      "epoch 791: w = 2.000, loss = 0.000000\n",
      "epoch 801: w = 2.000, loss = 0.000000\n",
      "epoch 811: w = 2.000, loss = 0.000000\n",
      "epoch 821: w = 2.000, loss = 0.000000\n",
      "epoch 831: w = 2.000, loss = 0.000000\n",
      "epoch 841: w = 2.000, loss = 0.000000\n",
      "epoch 851: w = 2.000, loss = 0.000000\n",
      "epoch 861: w = 2.000, loss = 0.000000\n",
      "epoch 871: w = 2.000, loss = 0.000000\n",
      "epoch 881: w = 2.000, loss = 0.000000\n",
      "epoch 891: w = 2.000, loss = 0.000000\n",
      "epoch 901: w = 2.000, loss = 0.000000\n",
      "epoch 911: w = 2.000, loss = 0.000000\n",
      "epoch 921: w = 2.000, loss = 0.000000\n",
      "epoch 931: w = 2.000, loss = 0.000000\n",
      "epoch 941: w = 2.000, loss = 0.000000\n",
      "epoch 951: w = 2.000, loss = 0.000000\n",
      "epoch 961: w = 2.000, loss = 0.000000\n",
      "epoch 971: w = 2.000, loss = 0.000000\n",
      "epoch 981: w = 2.000, loss = 0.000000\n",
      "epoch 991: w = 2.000, loss = 0.000000\n",
      "epoch 1001: w = 2.000, loss = 0.000000\n",
      "epoch 1011: w = 2.000, loss = 0.000000\n",
      "epoch 1021: w = 2.000, loss = 0.000000\n",
      "epoch 1031: w = 2.000, loss = 0.000000\n",
      "epoch 1041: w = 2.000, loss = 0.000000\n",
      "epoch 1051: w = 2.000, loss = 0.000000\n",
      "epoch 1061: w = 2.000, loss = 0.000000\n",
      "epoch 1071: w = 2.000, loss = 0.000000\n",
      "epoch 1081: w = 2.000, loss = 0.000000\n",
      "epoch 1091: w = 2.000, loss = 0.000000\n",
      "epoch 1101: w = 2.000, loss = 0.000000\n",
      "epoch 1111: w = 2.000, loss = 0.000000\n",
      "epoch 1121: w = 2.000, loss = 0.000000\n",
      "epoch 1131: w = 2.000, loss = 0.000000\n",
      "epoch 1141: w = 2.000, loss = 0.000000\n",
      "epoch 1151: w = 2.000, loss = 0.000000\n",
      "epoch 1161: w = 2.000, loss = 0.000000\n",
      "epoch 1171: w = 2.000, loss = 0.000000\n",
      "epoch 1181: w = 2.000, loss = 0.000000\n",
      "epoch 1191: w = 2.000, loss = 0.000000\n",
      "epoch 1201: w = 2.000, loss = 0.000000\n",
      "epoch 1211: w = 2.000, loss = 0.000000\n",
      "epoch 1221: w = 2.000, loss = 0.000000\n",
      "epoch 1231: w = 2.000, loss = 0.000000\n",
      "epoch 1241: w = 2.000, loss = 0.000000\n",
      "epoch 1251: w = 2.000, loss = 0.000000\n",
      "epoch 1261: w = 2.000, loss = 0.000000\n",
      "epoch 1271: w = 2.000, loss = 0.000000\n",
      "epoch 1281: w = 2.000, loss = 0.000000\n",
      "epoch 1291: w = 2.000, loss = 0.000000\n",
      "epoch 1301: w = 2.000, loss = 0.000000\n",
      "epoch 1311: w = 2.000, loss = 0.000000\n",
      "epoch 1321: w = 2.000, loss = 0.000000\n",
      "epoch 1331: w = 2.000, loss = 0.000000\n",
      "epoch 1341: w = 2.000, loss = 0.000000\n",
      "epoch 1351: w = 2.000, loss = 0.000000\n",
      "epoch 1361: w = 2.000, loss = 0.000000\n",
      "epoch 1371: w = 2.000, loss = 0.000000\n",
      "epoch 1381: w = 2.000, loss = 0.000000\n",
      "epoch 1391: w = 2.000, loss = 0.000000\n",
      "epoch 1401: w = 2.000, loss = 0.000000\n",
      "epoch 1411: w = 2.000, loss = 0.000000\n",
      "epoch 1421: w = 2.000, loss = 0.000000\n",
      "epoch 1431: w = 2.000, loss = 0.000000\n",
      "epoch 1441: w = 2.000, loss = 0.000000\n",
      "epoch 1451: w = 2.000, loss = 0.000000\n",
      "epoch 1461: w = 2.000, loss = 0.000000\n",
      "epoch 1471: w = 2.000, loss = 0.000000\n",
      "epoch 1481: w = 2.000, loss = 0.000000\n",
      "epoch 1491: w = 2.000, loss = 0.000000\n",
      "epoch 1501: w = 2.000, loss = 0.000000\n",
      "epoch 1511: w = 2.000, loss = 0.000000\n",
      "epoch 1521: w = 2.000, loss = 0.000000\n",
      "epoch 1531: w = 2.000, loss = 0.000000\n",
      "epoch 1541: w = 2.000, loss = 0.000000\n",
      "epoch 1551: w = 2.000, loss = 0.000000\n",
      "epoch 1561: w = 2.000, loss = 0.000000\n",
      "epoch 1571: w = 2.000, loss = 0.000000\n",
      "epoch 1581: w = 2.000, loss = 0.000000\n",
      "epoch 1591: w = 2.000, loss = 0.000000\n",
      "epoch 1601: w = 2.000, loss = 0.000000\n",
      "epoch 1611: w = 2.000, loss = 0.000000\n",
      "epoch 1621: w = 2.000, loss = 0.000000\n",
      "epoch 1631: w = 2.000, loss = 0.000000\n",
      "epoch 1641: w = 2.000, loss = 0.000000\n",
      "epoch 1651: w = 2.000, loss = 0.000000\n",
      "epoch 1661: w = 2.000, loss = 0.000000\n",
      "epoch 1671: w = 2.000, loss = 0.000000\n",
      "epoch 1681: w = 2.000, loss = 0.000000\n",
      "epoch 1691: w = 2.000, loss = 0.000000\n",
      "epoch 1701: w = 2.000, loss = 0.000000\n",
      "epoch 1711: w = 2.000, loss = 0.000000\n",
      "epoch 1721: w = 2.000, loss = 0.000000\n",
      "epoch 1731: w = 2.000, loss = 0.000000\n",
      "epoch 1741: w = 2.000, loss = 0.000000\n",
      "epoch 1751: w = 2.000, loss = 0.000000\n",
      "epoch 1761: w = 2.000, loss = 0.000000\n",
      "epoch 1771: w = 2.000, loss = 0.000000\n",
      "epoch 1781: w = 2.000, loss = 0.000000\n",
      "epoch 1791: w = 2.000, loss = 0.000000\n",
      "epoch 1801: w = 2.000, loss = 0.000000\n",
      "epoch 1811: w = 2.000, loss = 0.000000\n",
      "epoch 1821: w = 2.000, loss = 0.000000\n",
      "epoch 1831: w = 2.000, loss = 0.000000\n",
      "epoch 1841: w = 2.000, loss = 0.000000\n",
      "epoch 1851: w = 2.000, loss = 0.000000\n",
      "epoch 1861: w = 2.000, loss = 0.000000\n",
      "epoch 1871: w = 2.000, loss = 0.000000\n",
      "epoch 1881: w = 2.000, loss = 0.000000\n",
      "epoch 1891: w = 2.000, loss = 0.000000\n",
      "epoch 1901: w = 2.000, loss = 0.000000\n",
      "epoch 1911: w = 2.000, loss = 0.000000\n",
      "epoch 1921: w = 2.000, loss = 0.000000\n",
      "epoch 1931: w = 2.000, loss = 0.000000\n",
      "epoch 1941: w = 2.000, loss = 0.000000\n",
      "epoch 1951: w = 2.000, loss = 0.000000\n",
      "epoch 1961: w = 2.000, loss = 0.000000\n",
      "epoch 1971: w = 2.000, loss = 0.000000\n",
      "epoch 1981: w = 2.000, loss = 0.000000\n",
      "epoch 1991: w = 2.000, loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = 2 * X\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "number_of_samples, number_of_features = X.shape\n",
    "print(number_of_samples, number_of_features)\n",
    "input_size = number_of_features\n",
    "output_size = number_of_features\n",
    "model = nn.Linear(input_size, output_size)\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "learning_rate = 1e-1\n",
    "number_of_interation = 2000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "for epoch in range(number_of_interation):\n",
    "    y_pred = model(X)\n",
    "    l = loss(Y, y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    w.grad.zero_()\n",
    "    [w, b] = model.parameters()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 0.903\n",
      "Prediction after training: f(5) = 0.903\n",
      "epoch 1: w = 3.212, loss = 27.745468\n",
      "epoch 11: w = 2.062, loss = 0.011794\n",
      "epoch 21: w = 2.031, loss = 0.001397\n",
      "epoch 31: w = 2.022, loss = 0.000759\n",
      "epoch 41: w = 2.016, loss = 0.000413\n",
      "epoch 51: w = 2.012, loss = 0.000225\n",
      "epoch 61: w = 2.009, loss = 0.000122\n",
      "epoch 71: w = 2.007, loss = 0.000067\n",
      "epoch 81: w = 2.005, loss = 0.000036\n",
      "epoch 91: w = 2.004, loss = 0.000020\n",
      "epoch 101: w = 2.003, loss = 0.000011\n",
      "epoch 111: w = 2.002, loss = 0.000006\n",
      "epoch 121: w = 2.001, loss = 0.000003\n",
      "epoch 131: w = 2.001, loss = 0.000002\n",
      "epoch 141: w = 2.001, loss = 0.000001\n",
      "epoch 151: w = 2.001, loss = 0.000001\n",
      "epoch 161: w = 2.000, loss = 0.000000\n",
      "epoch 171: w = 2.000, loss = 0.000000\n",
      "epoch 181: w = 2.000, loss = 0.000000\n",
      "epoch 191: w = 2.000, loss = 0.000000\n",
      "epoch 201: w = 2.000, loss = 0.000000\n",
      "epoch 211: w = 2.000, loss = 0.000000\n",
      "epoch 221: w = 2.000, loss = 0.000000\n",
      "epoch 231: w = 2.000, loss = 0.000000\n",
      "epoch 241: w = 2.000, loss = 0.000000\n",
      "epoch 251: w = 2.000, loss = 0.000000\n",
      "epoch 261: w = 2.000, loss = 0.000000\n",
      "epoch 271: w = 2.000, loss = 0.000000\n",
      "epoch 281: w = 2.000, loss = 0.000000\n",
      "epoch 291: w = 2.000, loss = 0.000000\n",
      "epoch 301: w = 2.000, loss = 0.000000\n",
      "epoch 311: w = 2.000, loss = 0.000000\n",
      "epoch 321: w = 2.000, loss = 0.000000\n",
      "epoch 331: w = 2.000, loss = 0.000000\n",
      "epoch 341: w = 2.000, loss = 0.000000\n",
      "epoch 351: w = 2.000, loss = 0.000000\n",
      "epoch 361: w = 2.000, loss = 0.000000\n",
      "epoch 371: w = 2.000, loss = 0.000000\n",
      "epoch 381: w = 2.000, loss = 0.000000\n",
      "epoch 391: w = 2.000, loss = 0.000000\n",
      "epoch 401: w = 2.000, loss = 0.000000\n",
      "epoch 411: w = 2.000, loss = 0.000000\n",
      "epoch 421: w = 2.000, loss = 0.000000\n",
      "epoch 431: w = 2.000, loss = 0.000000\n",
      "epoch 441: w = 2.000, loss = 0.000000\n",
      "epoch 451: w = 2.000, loss = 0.000000\n",
      "epoch 461: w = 2.000, loss = 0.000000\n",
      "epoch 471: w = 2.000, loss = 0.000000\n",
      "epoch 481: w = 2.000, loss = 0.000000\n",
      "epoch 491: w = 2.000, loss = 0.000000\n",
      "epoch 501: w = 2.000, loss = 0.000000\n",
      "epoch 511: w = 2.000, loss = 0.000000\n",
      "epoch 521: w = 2.000, loss = 0.000000\n",
      "epoch 531: w = 2.000, loss = 0.000000\n",
      "epoch 541: w = 2.000, loss = 0.000000\n",
      "epoch 551: w = 2.000, loss = 0.000000\n",
      "epoch 561: w = 2.000, loss = 0.000000\n",
      "epoch 571: w = 2.000, loss = 0.000000\n",
      "epoch 581: w = 2.000, loss = 0.000000\n",
      "epoch 591: w = 2.000, loss = 0.000000\n",
      "epoch 601: w = 2.000, loss = 0.000000\n",
      "epoch 611: w = 2.000, loss = 0.000000\n",
      "epoch 621: w = 2.000, loss = 0.000000\n",
      "epoch 631: w = 2.000, loss = 0.000000\n",
      "epoch 641: w = 2.000, loss = 0.000000\n",
      "epoch 651: w = 2.000, loss = 0.000000\n",
      "epoch 661: w = 2.000, loss = 0.000000\n",
      "epoch 671: w = 2.000, loss = 0.000000\n",
      "epoch 681: w = 2.000, loss = 0.000000\n",
      "epoch 691: w = 2.000, loss = 0.000000\n",
      "epoch 701: w = 2.000, loss = 0.000000\n",
      "epoch 711: w = 2.000, loss = 0.000000\n",
      "epoch 721: w = 2.000, loss = 0.000000\n",
      "epoch 731: w = 2.000, loss = 0.000000\n",
      "epoch 741: w = 2.000, loss = 0.000000\n",
      "epoch 751: w = 2.000, loss = 0.000000\n",
      "epoch 761: w = 2.000, loss = 0.000000\n",
      "epoch 771: w = 2.000, loss = 0.000000\n",
      "epoch 781: w = 2.000, loss = 0.000000\n",
      "epoch 791: w = 2.000, loss = 0.000000\n",
      "epoch 801: w = 2.000, loss = 0.000000\n",
      "epoch 811: w = 2.000, loss = 0.000000\n",
      "epoch 821: w = 2.000, loss = 0.000000\n",
      "epoch 831: w = 2.000, loss = 0.000000\n",
      "epoch 841: w = 2.000, loss = 0.000000\n",
      "epoch 851: w = 2.000, loss = 0.000000\n",
      "epoch 861: w = 2.000, loss = 0.000000\n",
      "epoch 871: w = 2.000, loss = 0.000000\n",
      "epoch 881: w = 2.000, loss = 0.000000\n",
      "epoch 891: w = 2.000, loss = 0.000000\n",
      "epoch 901: w = 2.000, loss = 0.000000\n",
      "epoch 911: w = 2.000, loss = 0.000000\n",
      "epoch 921: w = 2.000, loss = 0.000000\n",
      "epoch 931: w = 2.000, loss = 0.000000\n",
      "epoch 941: w = 2.000, loss = 0.000000\n",
      "epoch 951: w = 2.000, loss = 0.000000\n",
      "epoch 961: w = 2.000, loss = 0.000000\n",
      "epoch 971: w = 2.000, loss = 0.000000\n",
      "epoch 981: w = 2.000, loss = 0.000000\n",
      "epoch 991: w = 2.000, loss = 0.000000\n",
      "epoch 1001: w = 2.000, loss = 0.000000\n",
      "epoch 1011: w = 2.000, loss = 0.000000\n",
      "epoch 1021: w = 2.000, loss = 0.000000\n",
      "epoch 1031: w = 2.000, loss = 0.000000\n",
      "epoch 1041: w = 2.000, loss = 0.000000\n",
      "epoch 1051: w = 2.000, loss = 0.000000\n",
      "epoch 1061: w = 2.000, loss = 0.000000\n",
      "epoch 1071: w = 2.000, loss = 0.000000\n",
      "epoch 1081: w = 2.000, loss = 0.000000\n",
      "epoch 1091: w = 2.000, loss = 0.000000\n",
      "epoch 1101: w = 2.000, loss = 0.000000\n",
      "epoch 1111: w = 2.000, loss = 0.000000\n",
      "epoch 1121: w = 2.000, loss = 0.000000\n",
      "epoch 1131: w = 2.000, loss = 0.000000\n",
      "epoch 1141: w = 2.000, loss = 0.000000\n",
      "epoch 1151: w = 2.000, loss = 0.000000\n",
      "epoch 1161: w = 2.000, loss = 0.000000\n",
      "epoch 1171: w = 2.000, loss = 0.000000\n",
      "epoch 1181: w = 2.000, loss = 0.000000\n",
      "epoch 1191: w = 2.000, loss = 0.000000\n",
      "epoch 1201: w = 2.000, loss = 0.000000\n",
      "epoch 1211: w = 2.000, loss = 0.000000\n",
      "epoch 1221: w = 2.000, loss = 0.000000\n",
      "epoch 1231: w = 2.000, loss = 0.000000\n",
      "epoch 1241: w = 2.000, loss = 0.000000\n",
      "epoch 1251: w = 2.000, loss = 0.000000\n",
      "epoch 1261: w = 2.000, loss = 0.000000\n",
      "epoch 1271: w = 2.000, loss = 0.000000\n",
      "epoch 1281: w = 2.000, loss = 0.000000\n",
      "epoch 1291: w = 2.000, loss = 0.000000\n",
      "epoch 1301: w = 2.000, loss = 0.000000\n",
      "epoch 1311: w = 2.000, loss = 0.000000\n",
      "epoch 1321: w = 2.000, loss = 0.000000\n",
      "epoch 1331: w = 2.000, loss = 0.000000\n",
      "epoch 1341: w = 2.000, loss = 0.000000\n",
      "epoch 1351: w = 2.000, loss = 0.000000\n",
      "epoch 1361: w = 2.000, loss = 0.000000\n",
      "epoch 1371: w = 2.000, loss = 0.000000\n",
      "epoch 1381: w = 2.000, loss = 0.000000\n",
      "epoch 1391: w = 2.000, loss = 0.000000\n",
      "epoch 1401: w = 2.000, loss = 0.000000\n",
      "epoch 1411: w = 2.000, loss = 0.000000\n",
      "epoch 1421: w = 2.000, loss = 0.000000\n",
      "epoch 1431: w = 2.000, loss = 0.000000\n",
      "epoch 1441: w = 2.000, loss = 0.000000\n",
      "epoch 1451: w = 2.000, loss = 0.000000\n",
      "epoch 1461: w = 2.000, loss = 0.000000\n",
      "epoch 1471: w = 2.000, loss = 0.000000\n",
      "epoch 1481: w = 2.000, loss = 0.000000\n",
      "epoch 1491: w = 2.000, loss = 0.000000\n",
      "epoch 1501: w = 2.000, loss = 0.000000\n",
      "epoch 1511: w = 2.000, loss = 0.000000\n",
      "epoch 1521: w = 2.000, loss = 0.000000\n",
      "epoch 1531: w = 2.000, loss = 0.000000\n",
      "epoch 1541: w = 2.000, loss = 0.000000\n",
      "epoch 1551: w = 2.000, loss = 0.000000\n",
      "epoch 1561: w = 2.000, loss = 0.000000\n",
      "epoch 1571: w = 2.000, loss = 0.000000\n",
      "epoch 1581: w = 2.000, loss = 0.000000\n",
      "epoch 1591: w = 2.000, loss = 0.000000\n",
      "epoch 1601: w = 2.000, loss = 0.000000\n",
      "epoch 1611: w = 2.000, loss = 0.000000\n",
      "epoch 1621: w = 2.000, loss = 0.000000\n",
      "epoch 1631: w = 2.000, loss = 0.000000\n",
      "epoch 1641: w = 2.000, loss = 0.000000\n",
      "epoch 1651: w = 2.000, loss = 0.000000\n",
      "epoch 1661: w = 2.000, loss = 0.000000\n",
      "epoch 1671: w = 2.000, loss = 0.000000\n",
      "epoch 1681: w = 2.000, loss = 0.000000\n",
      "epoch 1691: w = 2.000, loss = 0.000000\n",
      "epoch 1701: w = 2.000, loss = 0.000000\n",
      "epoch 1711: w = 2.000, loss = 0.000000\n",
      "epoch 1721: w = 2.000, loss = 0.000000\n",
      "epoch 1731: w = 2.000, loss = 0.000000\n",
      "epoch 1741: w = 2.000, loss = 0.000000\n",
      "epoch 1751: w = 2.000, loss = 0.000000\n",
      "epoch 1761: w = 2.000, loss = 0.000000\n",
      "epoch 1771: w = 2.000, loss = 0.000000\n",
      "epoch 1781: w = 2.000, loss = 0.000000\n",
      "epoch 1791: w = 2.000, loss = 0.000000\n",
      "epoch 1801: w = 2.000, loss = 0.000000\n",
      "epoch 1811: w = 2.000, loss = 0.000000\n",
      "epoch 1821: w = 2.000, loss = 0.000000\n",
      "epoch 1831: w = 2.000, loss = 0.000000\n",
      "epoch 1841: w = 2.000, loss = 0.000000\n",
      "epoch 1851: w = 2.000, loss = 0.000000\n",
      "epoch 1861: w = 2.000, loss = 0.000000\n",
      "epoch 1871: w = 2.000, loss = 0.000000\n",
      "epoch 1881: w = 2.000, loss = 0.000000\n",
      "epoch 1891: w = 2.000, loss = 0.000000\n",
      "epoch 1901: w = 2.000, loss = 0.000000\n",
      "epoch 1911: w = 2.000, loss = 0.000000\n",
      "epoch 1921: w = 2.000, loss = 0.000000\n",
      "epoch 1931: w = 2.000, loss = 0.000000\n",
      "epoch 1941: w = 2.000, loss = 0.000000\n",
      "epoch 1951: w = 2.000, loss = 0.000000\n",
      "epoch 1961: w = 2.000, loss = 0.000000\n",
      "epoch 1971: w = 2.000, loss = 0.000000\n",
      "epoch 1981: w = 2.000, loss = 0.000000\n",
      "epoch 1991: w = 2.000, loss = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = 2 * X\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "number_of_samples, number_of_features = X.shape\n",
    "print(number_of_samples, number_of_features)\n",
    "input_size = number_of_features\n",
    "output_size = number_of_features\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
    "learning_rate = 1e-1\n",
    "number_of_interation = 2000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")\n",
    "\n",
    "for epoch in range(number_of_interation):\n",
    "    y_pred = model(X)\n",
    "    l = loss(Y, y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    w.grad.zero_()\n",
    "    [w, b] = model.parameters()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre reshape y.shape = torch.Size([100])\n",
      "post reshape (y.view(y.shape[0], 1)): y.shape = torch.Size([100, 1])\n",
      "epoch: 200, loss = 342.4890\n",
      "epoch: 400, loss = 332.5870\n",
      "epoch: 600, loss = 332.5676\n",
      "epoch: 800, loss = 332.5676\n",
      "epoch: 1000, loss = 332.5676\n",
      "epoch: 1200, loss = 332.5676\n",
      "epoch: 1400, loss = 332.5676\n",
      "epoch: 1600, loss = 332.5676\n",
      "epoch: 1800, loss = 332.5676\n",
      "epoch: 2000, loss = 332.5676\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0. prepare data\n",
    "X_numpy, y_numpy = datasets.make_regression(\n",
    "    n_samples=100, n_features=1, noise=20, random_state=1\n",
    ")\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "print(f\"pre reshape y.shape = {y.size()}\")\n",
    "y = y.view(y.shape[0], 1)\n",
    "print(f\"post reshape (y.view(y.shape[0], 1)): y.shape = {y.size()}\")\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "# 1. model (linear regression (one layer))\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# 3. training loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    # bacward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # print some info\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"epoch: {epoch + 1}, loss = {loss.item():.4f}\")\n",
    "\n",
    "# plot\n",
    "predicted = model(X).detach().numpy()  # detach -- no grad?\n",
    "plt.plot(X_numpy, y_numpy, \"ro\")\n",
    "plt.plot(X_numpy, predicted, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 200 loss = 0.1839\n",
      "epoch: 400 loss = 0.1338\n",
      "epoch: 600 loss = 0.1122\n",
      "epoch: 800 loss = 0.0997\n",
      "epoch: 1000 loss = 0.09126\n",
      "epoch: 1200 loss = 0.08507\n",
      "epoch: 1400 loss = 0.08027\n",
      "epoch: 1600 loss = 0.07641\n",
      "epoch: 1800 loss = 0.0732\n",
      "epoch: 2000 loss = 0.07048\n",
      "Accuracy = 0.9561\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0. prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")  # 20% -- test\n",
    "# scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# 1. model\n",
    "# f = wx + b, sigmoid at the end\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features) -> None:\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2. loss and optimizer\n",
    "learning_rate = 1e-2\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3. training loop\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    # forward and loss\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # updates\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"epoch: {epoch + 1} loss = {loss.item():.4}\")\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_predicted_cls = y_pred.round()  # 0 - 1\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f\"Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches and load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch: 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch: 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch: 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch: 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        # data loading\n",
    "        xy = np.loadtxt(\"./data/wine.csv\", delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]])  # n_samples, 1 new array\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "dataset = WineDataset()\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset, batch_size=4, shuffle=True, num_workers=12\n",
    ")  # batch size = 4\n",
    "# dataiter = iter(dataloader)\n",
    "# data = dataiter.next()\n",
    "# features, labels = data\n",
    "# print(features, labels)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples / float(4))\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # forward bacward, update\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch + 1}/{num_epochs}, step {i + 1}/{n_iterations}, inputs {inputs.shape}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset transorms\n",
    "\n",
    "##      ,    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
      "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
      "        2.1300e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, transform=None) -> None:\n",
    "        # data loading\n",
    "        xy = np.loadtxt(\"./data/wine.csv\", delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        self.x = xy[:, 1:]\n",
    "        self.y = xy[:, [0]]  # n_samples, 1 new array\n",
    "        self.n_samples = xy.shape[0]\n",
    "        self.transfom = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index], self.y[index]\n",
    "        if self.transfom:\n",
    "            sample = self.transfom(sample)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample) -> None:\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "\n",
    "\n",
    "class MulTransform:\n",
    "    def __init__(self, factor) -> None:\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs *= self.factor\n",
    "        return inputs, target\n",
    "\n",
    "\n",
    "dataset = WineDataset(transform=ToTensor())\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
    "dataset = WineDataset(transform=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax and Cross entropy\n",
    "\n",
    "cross entropy: $$ D(\\hat Y, Y) = - \\frac{1}{N} \\sum Y_i \\log(\\hat Y_i)$$\n",
    "softmax: $$ S(y_i) = \\frac{e^{y_i}}{\\sum e^{y_j}}$$ \n",
    "***\n",
    "**\n",
    "> 1. softmax     , ..      .\n",
    "\n",
    "> 2. Y -- not One-Hot!\n",
    "\n",
    "> 3. Y_pred -- no Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_numpy_\n",
    "``` python\n",
    "def cross_entropy(actual, predicted):\n",
    "    loss = - np.sum(actual * np.log(predicted))\n",
    "    return loss\n",
    "\n",
    "```\n",
    "\n",
    "``` python'\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0\n",
    "```\n",
    "_pytorch_\n",
    "``` python\n",
    "loss = nn.CrossEntropyLoss()\n",
    "softmax = torch.softmax\n",
    "```\n",
    "\n",
    "_example 0_\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0])\n",
    "# nsamples x nclasses = 1 * 3 --    3  \n",
    "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item(), l2.item())\n",
    "```\n",
    "_output_example_0_\n",
    "```\n",
    " 0.4170299470424652 1.840616226196289 \n",
    "```\n",
    "___\n",
    "_example 1_\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# samples\n",
    "Y = torch.tensor([2, 0, 1])\n",
    "# number of prognoses = nsamples x nclasses = 3 * 3 --    3  \n",
    "Y_pred_good = torch.tensor([[0.0, 1.0, 2.1], [2.0, 1.0, 0.1], [2.0, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [2.0, 3.0, 0.1]])\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item(), l2.item())\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, axis=1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, axis=1)\n",
    "print(predictions1, predictions2)\n",
    "```\n",
    "\n",
    "_output_example_1_\n",
    "```\n",
    " 0.4170299470424652 1.840616226196289 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38165321946144104 1.7069271802902222\n",
      "tensor([2, 0, 1]) tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# samples\n",
    "Y = torch.tensor([2, 0, 1])\n",
    "# number of prognoses = nsamples x nclasses = 3 * 3 --    3  \n",
    "Y_pred_good = torch.tensor([[0.0, 1.0, 2.1], [2.0, 1.0, 0.1], [2.0, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1], [0.1, 1.0, 2.1], [2.0, 3.0, 0.1]])\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item(), l2.item())\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, axis=1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, axis=1)\n",
    "print(predictions1, predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes) -> None:\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNet2(input_size=28 * 28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary class \n",
    "## yes or no "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size) -> None:\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = NeuralNet2(input_size=28 * 28, hidden_size=5)\n",
    "criterion = (\n",
    "    nn.BCELoss()\n",
    ")  #      (  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Network\n",
    "## MNIST database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(f\"device = {device}\")\n",
    "# hyper parameters\n",
    "input_size = 28 * 28  # image size 28 x 28 = 784\n",
    "hidden_size = 2000\n",
    "num_classes = 10  # 0 1 2 3 4 5 6 7 8 9\n",
    "num_epochs = 20  # or more\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "# MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\".data/\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\".data/\", train=False, transform=transforms.ToTensor(), download=False\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(samples[i, 0], cmap=\"gray\")\n",
    "    plt.xlabel(labels[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/20, step 100/600, loss = 0.1907\n",
      "epoch: 1/20, step 200/600, loss = 0.1724\n",
      "epoch: 1/20, step 300/600, loss = 0.0878\n",
      "epoch: 1/20, step 400/600, loss = 0.1381\n",
      "epoch: 1/20, step 500/600, loss = 0.0800\n",
      "epoch: 1/20, step 600/600, loss = 0.2884\n",
      "epoch: 2/20, step 100/600, loss = 0.1383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3021422/3878451500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mn_total_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;31m# 100, 1, 28, 28 -> 100, 784 => reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/neural_networks/-test-multy_cognitive_tasks/env/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"category\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             warnings.warn(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes) -> None:\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class NeuralNet2h(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes) -> None:\n",
    "        super(NeuralNet2h, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNet2h(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"lif.recurrent_weights\":\n",
    "        prev_weights = param\n",
    "# training loop\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 100, 1, 28, 28 -> 100, 784 => reshape\n",
    "        images = images.reshape(-1, 28 * 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"epoch: {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for image, labels in test_loader:\n",
    "        image = image.reshape(-1, 28 * 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(image)\n",
    "        # value, index. Our index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predictions == labels).sum().cpu().item()\n",
    "acc = 100.0 * n_correct / n_samples\n",
    "print(f\"accuracy = {acc}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"lif.recurrent_weights\":\n",
    "        post_weights = param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 0. prepare dataset\n",
    "# dataset has PILImage of range [0, 1]\n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=False, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "# 1. model\n",
    "class ConNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, 6, 5\n",
    "        )  # 3 color_channels, 5 -- core size and 6 -- output size\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # size and step\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "# 2. loss and optimizer\n",
    "\n",
    "# 3. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from norse.torch import LICell  # Leaky integrator\n",
    "from norse.torch import LIFCell  # Leaky integrate-and-fire\n",
    "from norse.torch import SequentialState  # Stateful sequential layers\n",
    "\n",
    "model = SequentialState(\n",
    "    nn.Conv2d(1, 20, 5, 1),  # Convolve from 1 -> 20 channels\n",
    "    LIFCell(),  # Spiking activation layer\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(20, 50, 5, 1),  # Convolve from 20 -> 50 channels\n",
    "    LIFCell(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),  # Flatten to 800 units\n",
    "    nn.Linear(800, 10),\n",
    "    LICell(),  # Non-spiking integrator layer\n",
    ")\n",
    "\n",
    "data = torch.randn(8, 1, 28, 28)  # 8 batches, 1 channel, 28x28 pixels\n",
    "output, state = model(data)  # Provides a tuple (tensor (8, 10), neuron state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LIFFeedForwardState' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2702347/1113923853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LIFFeedForwardState' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import norse\n",
    "import torch\n",
    "import norse.torch as snn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = torch.randn(100000, 1, 20) * 1  # 10 timesteps, 1 bathces, 2 neurons\n",
    "voltage = np.zeros((4, data.shape[0]))\n",
    "spikes = np.zeros((4, data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa95eb94d00>]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = l(data)\n",
    "outputs[0].size()\n",
    "plt.plot(outputs[0][:, 0, 0].detach().cpu().numpy(), \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9037, -0.0672,  0.3875,  0.1483], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, state = l(data[i, ...], state=state)\n",
    "state[1].v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "l(data)[0].size()\n",
    "output, state = l(data[0, ...])\n",
    "%matplotlib\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(l(data)[0][:, 0, i].detach().numpy(), \".\")\n",
    "    plt.plot(voltage[i])\n",
    "    plt.xlabel(i + 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2702347/1355661624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjust_lif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLIF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurent_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "just_lif = norse.torch.LIF(2, 4, recurrent_weights=recurent_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2564, -0.3798,  0.0463, -0.3936]], grad_fn=<AddmmBackward0>),\n",
       " [None,\n",
       "  LIFState(z=tensor([0., 0., 0., 0.], grad_fn=<CppNode<SuperFunction>>), v=tensor([0., 0., 0., 0.], grad_fn=<AddBackward0>), i=tensor([0.4378, 0.1181, 0.0123, 0.3152], grad_fn=<AddBackward0>)),\n",
       "  None])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, state = l(data[0, ...])\n",
    "l(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(data[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "996bd9354ccb6f80ad6e6c455c9578561fe7b19b1b69868c4d5419a2d2220e9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
